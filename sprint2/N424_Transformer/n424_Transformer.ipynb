{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n424_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ixL-TF_uWx"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## ***AIB / SECTION 4 / SPRINT 2 / NOTE 4***\n",
        "\n",
        "---\n",
        "\n",
        "# Transformer & BERT, GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8x1QDTUe3cD"
      },
      "source": [
        "## ğŸ† í•™ìŠµ ëª©í‘œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lc3U9wBSrg"
      },
      "source": [
        "- **Transformerì˜ ì¥ì ê³¼ ì£¼ìš” í”„ë¡œì„¸ìŠ¤ì¸ Self-Attentionì— ëŒ€í•´ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.** \n",
        "    - íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ë°œí‘œí•œ ë…¼ë¬¸ ì œëª©ì€ ì™œ \"Attention is All You Need\"ì¸ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
        "    - Positional Encodingì„ ì ìš©í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
        "    - Masked Self-Attentionê°€ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ì¤‘ ì–´ë””ì— ì ìš©ë˜ë©° ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. \n",
        "    - ê¸°ì¡´ RNNê³¼ ë¹„êµí•˜ì—¬ Transformerê°€ ê°€ì§€ëŠ” ì¥ì ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "- **GPT, BERT ê·¸ë¦¬ê³  ë‹¤ë¥¸ ëª¨ë¸ì— ëŒ€í•´ì„œ ê°œëµì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.**\n",
        "    - GPT(Generative Pre-Training)\n",
        "        - ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸(Pre-trained LM)ì˜ Pre-trainingê³¼ Fine-tuningì€ ë¬´ì—‡ì´ê³  ê°ê° ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
        "        - GPTëŠ” Transformerë¥¼ ì–´ë–»ê²Œ ë³€í˜•í•˜ì˜€ëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.\n",
        "    - BERT(Bidirectional Encoder Representation by Transformer)\n",
        "        - BERTëŠ” Transformerë¥¼ ì–´ë–»ê²Œ ë³€í˜•í•˜ì˜€ìœ¼ë©° GPTì™€ì˜ ì°¨ì´ ë¬´ì—‡ì¸ì§€ ì•Œ ìˆ˜ ìˆë‹¤.\n",
        "        - MLM(Masked Language Model)ì€ ë¬´ì—‡ì¸ì§€ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n",
        "        - NSP(Next Sentence Prediction)ì€ ë¬´ì—‡ì¸ì§€ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n",
        "- **ìµœê·¼ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ì€ ì–´ë–»ê²Œ ì§„í–‰ë˜ê³  ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.**\n",
        "\n",
        "- ì£¼ì˜ : ì´í›„ì—ë„ ìƒˆë¡œìš´ ëª¨ë¸ì„ ê³µë¶€í•˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ í•™ìŠµí•˜ê²Œ ë  ê²ë‹ˆë‹¤.\n",
        "    1. ë…¼ë¬¸ì„ í†µí•´ í•´ë‹¹ ëª¨ë¸ì˜ ì»¨ì…‰ê³¼ ëŒ€ëµì ì¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•œë‹¤.\n",
        "    2. ê¸°ì¡´ì— êµ¬í˜„ë˜ì–´ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í•´ë‹¹ ëª¨ë¸ì„ ì‚¬ìš©í•´ë³¸ë‹¤. (ì¶”ë¡  í˜¹ì€ íŒŒì¸íŠœë‹)\n",
        "    3. í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•œ ì¢€ ë” ê¹Šì€ ì´í•´ê°€ í•„ìš”í•  ê²½ìš° ì½”ë“œë¡œ êµ¬í˜„í•´ë³¸ë‹¤.\n",
        "    - ì¦‰, ì˜¤ëŠ˜ì€ ì²« ë²ˆì§¸ë¡œ ì•„ë˜ ê°œë…ì„ ë°°ìš°ëŠ” ë§Œí¼ ëª¨ë“  ì½”ë“œë¥¼ ì´í•´í•˜ì§€ ëª»í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb9w_Bwle7tv"
      },
      "source": [
        "## ğŸ›« Warm Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwcgIeUuz3OK"
      },
      "source": [
        "ì§€ë‚œ ì‹œê°„ì— ë°°ì› ë˜ ë‚´ìš©ì„ ë– ì˜¬ë ¤ë´…ì‹œë‹¤.\n",
        "\n",
        "- **RNN, LSTM, GRU**\n",
        "\n",
        "    - RNN ê¸°ë°˜ ëª¨ë¸ì˜ ì¥ì ì— ëŒ€í•´ì„œ ìƒê°í•´ë´…ì‹œë‹¤.\n",
        "    - RNN ê¸°ë°˜ ëª¨ë¸ì˜ ë‹¨ì ì— ëŒ€í•´ì„œ ìƒê°í•´ë´…ì‹œë‹¤. (2ê°€ì§€ ì´ìƒ)\n",
        "        - LSTMê³¼ GRUëŠ” ì–´ë–¤ ë‹¨ì ì„ ì–´ë–»ê²Œ ê·¹ë³µí•˜ì˜€ëŠ”ì§€ ë‹¤ì‹œ ì•Œì•„ë´…ì‹œë‹¤.\n",
        "\n",
        "- ì´ë²ˆ ì‹œê°„ì—ëŠ” **Transformer**ì— ëŒ€í•´ì„œ ë°°ìš¸ ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "    - [Transformer](https://www.youtube.com/watch?v=mxGCEWOxfe8) ì†Œê°œ ì˜ìƒ\n",
        "        - RNNê³¼ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ëª¨ë¸\n",
        "        - ***Attention is All You Need : í•„ìš”í•œ ê±´ Attention ë¿***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpSQ4utmO71g"
      },
      "source": [
        "## 1. Transformer : Attention is All You Need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpodce2C3xqS"
      },
      "source": [
        "### íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ë€?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Nkvve_FFIG"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/0/0f/Transformers_Age_of_Extinction_poster.jpg/220px-Transformers_Age_of_Extinction_poster.jpg\" alt=\"transformer_electric\" width=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKH7yo-AO3Ph"
      },
      "source": [
        "**<font color=\"ff6f61\">íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)</font>**ëŠ” ê¸°ê³„ ë²ˆì—­ì„ ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì´ì „ì— ë“±ì¥í–ˆë˜ **Attention ë©”ì»¤ë‹ˆì¦˜ì„ ê·¹ëŒ€í™”**í•˜ì—¬ ë›°ì–´ë‚œ ë²ˆì—­ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.<br/>\n",
        "ìµœê·¼ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ SOTA(State-of-the-Art)ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ê±°ì˜ ëª¨ë‘ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "ëª¨ë¸ì„ ì†Œê°œí•œ ë…¼ë¬¸ [Attention is All You Need](https://arxiv.org/abs/1706.03762) ëŠ” 3ë…„ ì‚¬ì´ì— 18000ë²ˆ ì´ìƒ ì¸ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.<br/>\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ìì—°ì–´ì²˜ë¦¬ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë¬¸ì œë„ ì˜ í’€ê³ ìˆê¸° ë•Œë¬¸ì— ìµœê·¼ì—ëŠ” ì»´í“¨í„° ë¹„ì „ ìª½ì—ì„œë„ ì ìš©í•˜ë ¤ëŠ” ì‹œë„ê°€ ìˆìœ¼ë©°, ë©€í‹°ëª¨ë‹¬(Multi-Modal) ëª¨ë¸ì—ë„ ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "\n",
        "RNN ê¸°ë°˜ ëª¨ë¸ì´ ê°€ì§„ êµ¬ì¡°ì  ë‹¨ì ì€ ë‹¨ì–´ê°€ **ìˆœì„œëŒ€ë¡œ** ë“¤ì–´ì˜¨ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.<br/>\n",
        "ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ì‹œí€€ìŠ¤ê°€ ê¸¸ìˆ˜ë¡ **ì—°ì‚° ì‹œê°„ì´** ê¸¸ì–´ì§‘ë‹ˆë‹¤.<br/>\n",
        "**íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ëª¨ë¸**ì…ë‹ˆë‹¤.<br/>\n",
        "ëª¨ë“  í† í°ì„ ë™ì‹œì— ì…ë ¥ë°›ì•„ ë³‘ë ¬ ì—°ì‚°í•˜ê¸° ë•Œë¬¸ì— GPU ì—°ì‚°ì— ìµœì í™” ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí•˜ê²Œ ì‹œê°í™”í•œ ê·¸ë¦¼ì…ë‹ˆë‹¤.<br/>\n",
        "Encoder, Decoderë¡œ í‘œí˜„ëœ ì‚¬ê°í˜•ì„ ê°ê° ì¸ì½”ë” ë¸”ë¡ê³¼ ë””ì½”ë” ë¸”ë¡ì´ë¼ê³  í•©ë‹ˆë‹¤.<br/>\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” **ì¸ì½”ë” ë¸”ë¡ê³¼ ë””ì½”ë” ë¸”ë¡ì´ 6ê°œì”©** ëª¨ì—¬ìˆëŠ” êµ¬ì¡°ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEjnrUq-Zq9X"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" alt=\"positional_encoding\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNUm_DCUO_Yn"
      },
      "source": [
        "ê·¸ë¦¼ í•˜ë‚˜ë¥¼ ë” ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ë…¼ë¬¸ì—ì„œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì…ë‹ˆë‹¤.<br/>\n",
        "ê·¸ë¦¼ì„ ë³´ë©´ ì»¤ë‹¤ë€ íšŒìƒ‰ ë¸”ë¡ì´ 2ê°œ ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "ì™¼ìª½ì€ ì¸ì½”ë” ë¸”ë¡ í•˜ë‚˜ë¥¼ ë‚˜íƒ€ë‚´ê³  ì˜¤ë¥¸ìª½ì€ ë””ì½”ë” ë¸”ë¡ í•˜ë‚˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.<br/>\n",
        "ì¸ì½”ë” ë¸”ë¡ì€ í¬ê²Œ **2ê°œì˜ sub-layer â–¶ï¸ [`Multi-Head (Self) Attention`, `Feed Forward`]** ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "ë””ì½”ë” ë¸”ë¡ì€ **3ê°œì˜ sub-layer â–¶ï¸ [`Masked Multi-Head (Self) Attention`, `Multi-Head (Encoder-Decoder) Attention`, `Feed Forward`]** ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> â—ï¸ ***ì•„ë˜ì— ë“±ì¥í•˜ëŠ” ê·¸ë¦¼ì€ Transformer ì˜ êµ¬ì¡°ë¥¼ ê°„ë‹¨í•˜ê²Œ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì…ë‹ˆë‹¤.<br/>\n",
        "ì´ë²ˆ ê°•ì˜ì—ì„œ ê³„ì† ë“±ì¥í•˜ê²Œ ë˜ë‹ˆ ì˜ ê¸°ì–µí•´ë‘ë©´ ì¢‹ê² ì£ ?***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf5KKTsuPENw"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" alt=\"positional_encoding\" width=\"550\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntqpucgWuOKh"
      },
      "source": [
        "### Positional Encoding (ìœ„ì¹˜ ì¸ì½”ë”©)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO2N2G4pPiA3"
      },
      "source": [
        "<img width=\"300\" alt=\"pe\" src=\"https://user-images.githubusercontent.com/45377884/112799904-ecb3a100-90a9-11eb-9072-87a965e81a77.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw_uLipBPnMG"
      },
      "source": [
        "íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ë³‘ë ¬í™”ë¥¼ ìœ„í•´ ëª¨ë“  ë‹¨ì–´ ë²¡í„°ë¥¼ ë™ì‹œì— ì…ë ¥ë°›ìŠµë‹ˆë‹¤.<br/>\n",
        "ì»´í“¨í„°ëŠ” ì–´ë–¤ ë‹¨ì–´ê°€ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤.<br/>\n",
        "ê·¸ë˜ì„œ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ë²¡í„°ë¥¼ ë”°ë¡œ ì œê³µí•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.<br/>\n",
        "ë‹¨ì–´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ë²¡í„°ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì„ **<font color=\"ff6f61\">Positional Encoding</font>** ì´ë¼ê³  í•©ë‹ˆë‹¤.<br/>\n",
        "\n",
        "Positional Encoding ì€ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.<br/>\n",
        "\n",
        "> â—ï¸ ***ìˆ˜ì‹ì´ ë³µì¡í•˜ë‹ˆ ìˆ˜í•™ì ìœ¼ë¡œ ì´í•´í•˜ë ¤ê³  í•˜ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤.<br/>\n",
        "ìˆ˜ì‹ë³´ë‹¤ëŠ” Positional Encodingì´ ì™œ í•„ìš”í•œì§€ë¥¼ ê¸°ì–µí•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•©ë‹ˆë‹¤.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQLVS_0zPpdA"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{PE}_{\\text{pos},2i} &= \\sin \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg) \\\\\n",
        "\\text{PE}_{\\text{pos},2i+1} &= \\cos \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg)\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqVM-W4y3cCG"
      },
      "source": [
        "ì•„ë˜ëŠ” Positional Encodingì„ í†µí•´ ë§Œë“¤ì–´ì§„ ë²¡í„°ë¥¼ ì‹œê°í™”í•œ ìë£Œì…ë‹ˆë‹¤.<br/>\n",
        "ì¼ì •í•œ íŒ¨í„´ì´ ìˆëŠ” ë²¡í„°ê°€ ë§Œë“¤ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br/>\n",
        "ì»´í“¨í„°ëŠ” ì´ë¥¼ í†µí•´ ë‹¨ì–´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ê²Œ ë©ë‹ˆë‹¤.<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZXLdjSfPva1"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" alt=\"positional_encoding\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO2S2h7q-vKE"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    \"\"\"\n",
        "    sin, cos ì•ˆì— ë“¤ì–´ê°ˆ ìˆ˜ì¹˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvW7dueW-zCK"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    \"\"\"\n",
        "    ìœ„ì¹˜ ì¸ì½”ë”©(Positional Encoding)ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "    \n",
        "    \"\"\"\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yD70CNguVAC"
      },
      "source": [
        "### Self-Attention (ì…€í”„-ì–´í…ì…˜)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykcWMWKwqP8-"
      },
      "source": [
        "> â—ï¸ ***ì´ë²ˆ ë…¸íŠ¸ì—ì„œëŠ” ì´ê²ƒë§Œì´ë¼ë„ ì•Œê³  ë„˜ì–´ê°€ë³´ë„ë¡ í•©ì‹œë‹¤.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gEVfOCmPycD"
      },
      "source": [
        "<img width=\"300\" alt=\"self-Attn\" src=\"https://user-images.githubusercontent.com/45377884/112809266-ca735080-90b4-11eb-9a25-7f34f37880c7.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2AxXXfPP0wH"
      },
      "source": [
        "íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì£¼ìš” ë©”ì»¤ë‹ˆì¦˜ì¸ **<font color=\"ff6f61\">Self-Attention</font>** ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ì™€ ê°™ì€ ë¬¸ì¥ì´ ìˆë‹¤ê³  í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "> *The animal didn't cross the street because <ins>it</ins> was too tired* \n",
        "\n",
        "ìœ„ ë¬¸ì¥ì„ ì œëŒ€ë¡œ ë²ˆì—­í•˜ë ¤ë©´ **_\"it\"_** ê³¼ ê°™ì€ ì§€ì‹œëŒ€ëª…ì‚¬ê°€ ì–´ë–¤ ëŒ€ìƒì„ ê°€ë¦¬í‚¤ëŠ”ì§€ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.<br/>\n",
        "ê·¸ë ‡ê¸° ë•Œë¬¸ì— íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ë²ˆì—­í•˜ë ¤ëŠ” ë¬¸ì¥ ë‚´ë¶€ ìš”ì†Œì˜ ê´€ê³„ë¥¼ ì˜ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œ, ë¬¸ì¥ ìì‹ ì— ëŒ€í•´ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "ì´ë¥¼ **Self-Attention** ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ëŠ” **_\"it\"_** ì´ ì–´ë–¤ ë‹¨ì–´ì™€ ê°€ì¥ ì—°ê´€ë˜ì–´ ìˆëŠ” ì§€ë¥¼ ì‹œê°í™”í•œ ê·¸ë¦¼ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvAsFAWJP2xz"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" alt=\"self_attention_visualization\" width=\"350\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG-Z6J9qwpyL"
      },
      "source": [
        "**Self-Attention**ì€ ì–´ë–¤ ê³¼ì •ì´ê¸¸ë˜ ë‹¨ì–´ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆì„ê¹Œìš”?\n",
        "\n",
        "Self-Attentionì—ì„œë„ ì¿¼ë¦¬(Query)-í‚¤(Key)-ë°¸ë¥˜(Value)ì˜ ì•„ì´ë””ì–´ê°€ ë™ì¼í•˜ê²Œ ë“±ì¥í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6au7EnYMtpi"
      },
      "source": [
        "- **ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ ë‚´ìš©ì„ ë³µìŠµí•´ë´…ì‹œë‹¤ - ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ í˜ëŸ¬ë‚˜ì˜¨ ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜**\n",
        "\n",
        "Attentionì„ ë‹¤ë£° ë•Œ ë“±ì¥í–ˆë˜ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì•„ì´ë””ì–´ë¥¼ ë‹¤ì‹œ ë³µìŠµí•´ë´…ì‹œë‹¤.\n",
        "\n",
        "ì•„ë˜ëŠ” êµ¬ê¸€ì—ì„œ _\"what is attention in nlp\"_ ë¼ëŠ” ê²€ìƒ‰ì–´ë¥¼ êµ¬ê¸€ì— ì…ë ¥í–ˆì„ ë•Œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ ì´ë¯¸ì§€ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gilv5vZHmTU6"
      },
      "source": [
        "<img src=\"https://i.imgur.com/JdCQr1l.png\" alt=\"retrieval_system\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NywtmkKKmS9c"
      },
      "source": [
        "ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ê²€ìƒ‰ ì‹œìŠ¤í…œì€ ì•„ë˜ì™€ ê°™ì€ 3ë‹¨ê³„ë¥¼ ê±°ì³ ì‘ë™í•©ë‹ˆë‹¤.\n",
        "\n",
        "1. ì°¾ê³ ì í•˜ëŠ” ì •ë³´ì— ëŒ€í•œ ê²€ìƒ‰ì–´(Query)ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\n",
        "2. ê²€ìƒ‰ ì—”ì§„ì€ ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ë¹„ìŠ·í•œ í‚¤ì›Œë“œ(Key)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
        "3. ê·¸ë¦¬ê³  í•´ë‹¹ í‚¤ì›Œë“œ(Key)ì™€ ì—°ê²°ëœ í˜ì´ì§€(Value)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnS4uzQOMsFO"
      },
      "source": [
        "**ê¸°ì¡´ Attentionê³¼ì˜ ì°¨ì´ëŠ” ê° ë²¡í„°ê°€ ëª¨ë‘ ê°€ì¤‘ì¹˜ ë²¡í„°ë¼ëŠ” ì **ì…ë‹ˆë‹¤.<br/>\n",
        "\n",
        "ê°ê°ì˜ ë²¡í„°ê°€ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- **ì¿¼ë¦¬(q)**ëŠ” ë¶„ì„í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ë²¡í„°ì…ë‹ˆë‹¤.\n",
        "\n",
        "- **í‚¤(k)**ëŠ” ê° ë‹¨ì–´ê°€ ì¿¼ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì™€ ì–¼ë§ˆë‚˜ ì—°ê´€ìˆëŠ” ì§€ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ë²¡í„°ì…ë‹ˆë‹¤.\n",
        "\n",
        "- **ë°¸ë¥˜(v)**ëŠ” ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì‚´ë ¤ì£¼ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ë²¡í„°ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrXC04GQy4Yb"
      },
      "source": [
        "**Self-Attention**ì€ ì„¸ ê°€ì§€ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì–´í…ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "ì ìš©í•˜ëŠ” ë°©ì‹ì€ ê¸°ì¡´ Attention ë©”ì»¤ë‹ˆì¦˜ê³¼ ê±°ì˜ ë™ì¼í•©ë‹ˆë‹¤.\n",
        "\n",
        "1. ë¨¼ì €, **íŠ¹ì • ë‹¨ì–´ì˜ ì¿¼ë¦¬(q) ë²¡í„°ì™€ ëª¨ë“  ë‹¨ì–´ì˜ í‚¤(k) ë²¡í„°ë¥¼ ë‚´ì **í•©ë‹ˆë‹¤. ë‚´ì ì„ í†µí•´ ë‚˜ì˜¤ëŠ” ê°’ì´ Attention ìŠ¤ì½”ì–´(Score)ê°€ ë©ë‹ˆë‹¤.\n",
        "\n",
        "2. íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ì´ ê°€ì¤‘ì¹˜ë¥¼ q,k,v ë²¡í„° ì°¨ì› $d_k$ ì˜ ì œê³±ê·¼ì¸ $\\sqrt{d_k}$ ë¡œ ë‚˜ëˆ„ì–´ì¤ë‹ˆë‹¤.<br/>ê³„ì‚°ê°’ì„ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ê¸° ìœ„í•œ ê³„ì‚° ë³´ì •ìœ¼ë¡œ ìƒê°í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.  \n",
        "\n",
        "3. ë‹¤ìŒìœ¼ë¡œ **Softmax**ë¥¼ ì·¨í•´ì¤ë‹ˆë‹¤.<br/>\n",
        "ì´ë¥¼ í†µí•´ ì¿¼ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì™€ ë¬¸ì¥ ë‚´ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ê°€ì§€ëŠ” ê´€ê³„ì˜ ë¹„ìœ¨ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "4. ë§ˆì§€ë§‰ìœ¼ë¡œ **ë°¸ë¥˜(v) ê° ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ê³±í•´ì¤€ í›„ ëª¨ë‘ ë”í•˜ë©´** Self-Attention ê³¼ì •ì´ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XUQi85yy7kR"
      },
      "source": [
        "**Self-Attention** ì˜ ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‹¤ì‹œ ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTREy_tA1tAc"
      },
      "source": [
        "**1. ê°€ì¤‘ì¹˜ í–‰ë ¬ $W^Q, W^K, W^V$ ë¡œë¶€í„° ê° ë‹¨ì–´ì˜ ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜(q, k, v) ë²¡í„°ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBOvyUgjRE6C"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-1.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH4SjvhPQB0h"
      },
      "source": [
        "**2. ë¶„ì„í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ì˜ ì¿¼ë¦¬ ë²¡í„°(q)ì™€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´(ìì‹  í¬í•¨)ì˜ í‚¤ ë²¡í„°(k)ë¥¼ ë‚´ì í•˜ì—¬ ê° ë‹¨ì–´ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ì •ë„ë¥¼ êµ¬í•©ë‹ˆë‹¤.**\n",
        "\n",
        "(ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” $\\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ì–´ ì¤€ ë’¤ì— Softmaxë¥¼ ì·¨í•´ì£¼ëŠ” ê³¼ì •ì€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKrYd1eWRIH7"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-2.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ_HnY9vQD4o"
      },
      "source": [
        "**3.  Softmaxì˜ ì¶œë ¥ê°’ê³¼ë°¸ë¥˜ ë²¡í„°(v)ë¥¼ ê³±í•´ì¤€ ë’¤ ë”í•˜ë©´ í•´ë‹¹ ë‹¨ì–´ì— ëŒ€í•œ Self-Attention ì¶œë ¥ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKbpOuTVRKf0"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-3.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iMflDTbQFV4"
      },
      "source": [
        "**4. í•˜ë‚˜ì˜ ë²¡í„°ì— ëŒ€í•´ì„œë§Œ ì‚´í´ë³´ì•˜ì§€ë§Œ ì‹¤ì œ Attention ê³„ì‚°ì€ í–‰ë ¬ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ê³„ì‚°ë©ë‹ˆë‹¤.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMO4BdoQRM7P"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-summary.png\" alt=\"transformer_15\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj6LqffDxYZu"
      },
      "source": [
        "ì‹¤ì œë¡œ ê° ë²¡í„°ëŠ” **í–‰ë ¬(Q, K, V)**ë¡œ í•œêº¼ë²ˆì— ê³„ì‚°ë©ë‹ˆë‹¤. $W^Q, W^K, W^V$ ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ê°±ì‹ ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ì…ë‹ˆë‹¤.<br/>\n",
        "ì„¸ í–‰ë ¬ê³¼ ë‹¨ì–´ í–‰ë ¬ì„ ë‚´ì í•˜ì—¬ ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜ í–‰ë ¬(Q, K, V)ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un25Mx6_RPem"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" alt=\"transformer_12\" width=\"400\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_CnhEVQI4h"
      },
      "source": [
        "ìœ„ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´\n",
        "\n",
        "1. ë¨¼ì € ì¿¼ë¦¬ í–‰ë ¬(Q)ê³¼ í‚¤ í–‰ë ¬(K)ì„ **ë‚´ì **í•©ë‹ˆë‹¤.\n",
        "\n",
        "2. ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” í–‰ë ¬ì˜ ìš”ì†Œë¥¼ $\\sqrt{d_k}$ ë¡œ **ë‚˜ëˆ„ì–´ ì¤ë‹ˆë‹¤.**\n",
        "\n",
        "3. í–‰ë ¬ì˜ ê° ìš”ì†Œì— **ì†Œí”„íŠ¸ë§¥ìŠ¤(Softmax)**ë¥¼ ì·¨í•´ì¤ë‹ˆë‹¤. \n",
        "\n",
        "4. ë§ˆì§€ë§‰ìœ¼ë¡œ **ë°¸ë¥˜ í–‰ë ¬(V)ê³¼ ë‚´ì **í•˜ë©´ ìµœì¢… ê²°ê³¼ í–‰ë ¬(Z)ì´ ë°˜í™˜ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinyklXWyLrn"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" alt=\"transformer_13\" width=\"700\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpxgcGIgj5PC"
      },
      "source": [
        "ì•„ë˜ëŠ” Tensorflow ì—ì„œ Self-Attentionì„ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤. ì½”ë“œë¥¼ í†µí•´ Attentionì´ ê³„ì‚°ë˜ëŠ” ê³¼ì •ì„ ë‹¤ì‹œ ì‚´í´ë³´ë„ë¡ í•©ì‹œë‹¤.\n",
        "\n",
        "> â—ï¸ ***ëª¨ë“  ì½”ë“œë¥¼ ë°”ë¡œ ì‘ì„±í•  ìˆ˜ ì—†ì–´ë„ ì¢‹ìŠµë‹ˆë‹¤.<br/>\n",
        "ë‹¹ì¥ì€ ì½”ë“œì™€ ì£¼ì„ì„ ë”°ë¼ê°€ë³´ë©´ì„œ Attention ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì— ëŒ€í•´ ì´í•´í•´ë´…ì‹œë‹¤.<br/>\n",
        "ì¶”ê°€ì ìœ¼ë¡œ ì•„ë˜ì— ë“±ì¥í•˜ëŠ” ì½”ë“œëŠ” Tensorflow ê³µì‹ ì½”ë“œì´ë¯€ë¡œ ì¶”í›„ ì•„ë˜ì˜ ì½”ë”© ìŠ¤íƒ€ì¼ì„ ì°¸ê³ í•´ë³´ì•„ë„ ì¢‹ê² ì£ ?***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFkqQMjHpIxI"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Attention ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "    q, k, v ì˜ leading dimensionì€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    k, vì˜ penultimate dimensionì´ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤, i.e.: seq_len_k = seq_len_v.\n",
        "\n",
        "    MaskëŠ” íƒ€ì…(padding or look ahead)ì— ë”°ë¼ ë‹¤ë¥¸ ì°¨ì›ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    ë§ì…ˆì‹œì—ëŠ” ë¸Œë¡œë“œìºìŠ¤íŒ… ë  ìˆ˜ ìˆì–´ì•¼í•©ë‹ˆë‹¤.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    # matmul_qk(ì¿¼ë¦¬ì™€ í‚¤ì˜ ë‚´ì )ì„ dkì˜ ì œê³±ê·¼ìœ¼ë¡œ scaling í•©ë‹ˆë‹¤.\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # ë§ˆìŠ¤í‚¹ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) í•¨ìˆ˜ë¥¼ í†µí•´ì„œ attention weight ë¥¼ êµ¬í•´ë´…ì‹œë‹¤.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_RmjSVYluaO"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTRaQ5oVQ1gh"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œ **<font color=\"ff6f61\">Multi-Head Attention</font>** ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.<br/>\n",
        "Multi-Head Attention ì´ë€ **Self-Attentionì„ ë™ì‹œì— ë³‘ë ¬ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.<br/>\n",
        "ê° Head ë§ˆë‹¤ ë‹¤ë¥¸ Attention ê²°ê³¼ë¥¼ ë‚´ì–´ì£¼ê¸° ë•Œë¬¸ì— ì•™ìƒë¸”ê³¼ ìœ ì‚¬í•œ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br/> \n",
        "ë…¼ë¬¸ì—ì„œëŠ” 8ê°œì˜ Headë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.<br/>\n",
        "8ë²ˆì˜ Self-Attentionì„ ì‹¤í–‰í•˜ì—¬ ê°ê°ì˜ ì¶œë ¥ í–‰ë ¬ $Z_0, Z_1, \\cdots , Z_7$ ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rvzeSQkQ4SJ"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_z.png\" alt=\"transformer_16\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTcLefocQ7mC"
      },
      "source": [
        "ì¶œë ¥ëœ í–‰ë ¬ $Z_n (n=0,\\cdots,7)$ ì€ **ì´ì–´ë¶™ì—¬ì§‘ë‹ˆë‹¤(Concatenate)**.<br/>\n",
        "ë˜ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° í–‰ë ¬ì¸ $W^o$ ì™€ì˜ ë‚´ì ì„ í†µí•´ Multi-Head Attentionì˜ ìµœì¢… ê²°ê³¼ì¸ í–‰ë ¬ $Z$ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.<br/>\n",
        "ì—¬ê¸°ì„œ í–‰ë ¬ $W^o$ì˜ ìš”ì†Œ ì—­ì‹œ í•™ìŠµì„ í†µí•´ ê°±ì‹ ë©ë‹ˆë‹¤.<br/>\n",
        "ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ í–‰ë ¬ $Z$ëŠ” í† í° ë²¡í„°ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ $X$ì™€ **ë™ì¼í•œ í¬ê¸°(Shape)**ê°€ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkS1oMy6Q-Pc"
      },
      "source": [
        "<img src=\"http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" alt=\"transformer_17\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZo2_s-Z2Vi2"
      },
      "source": [
        "### Layer Normalization & Skip Connection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLzwJ00c9zTV"
      },
      "source": [
        "<img width=\"300\" alt=\"lnorm_resicon\" src=\"https://user-images.githubusercontent.com/45377884/113169444-9056aa00-9280-11eb-8ba0-17c9211ad412.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL55-1zZ9zrS"
      },
      "source": [
        "íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ëª¨ë“  sub-layerì—ì„œ ì¶œë ¥ëœ ë²¡í„°ëŠ” **Layer normalization**ê³¼ **Skip connection**ì„ ê±°ì¹˜ê²Œ ë©ë‹ˆë‹¤.<br/>\n",
        "Layer normalizationì˜ íš¨ê³¼ëŠ” Batch normalizationê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. í•™ìŠµì´ í›¨ì”¬ ë¹ ë¥´ê³  ì˜ ë˜ë„ë¡ í•©ë‹ˆë‹¤.<br/>\n",
        "Skip connection(í˜¹ì€ Residual connection)ì€ ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì •ë³´ê°€ ì†Œì‹¤ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.<br/>\n",
        "Sprint 3ì—ì„œ ë°°ìš¸ ResNetì˜ ì£¼ìš” ë©”ì»¤ë‹ˆì¦˜ì´ë¯€ë¡œ í•´ë‹¹ ë¶€ë¶„ì—ì„œ ë”ìš± ìì„¸í•˜ê²Œ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQPefZjI2UTM"
      },
      "source": [
        "### Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHwk8VeqmNzU"
      },
      "source": [
        "<img width=\"300\" alt=\"á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-03-29 á„‹á…©á„’á…® 5 27 32\" src=\"https://user-images.githubusercontent.com/45377884/112808809-58027080-90b4-11eb-8ca7-ffa38e577d3d.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H4S8ETC8BMO"
      },
      "source": [
        "ë‹¤ìŒìœ¼ë¡œ **<font color=\"ff6f61\">FFNN(Feed forward neural network)</font>** ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.<br/>ì€ë‹‰ì¸µì˜ ì°¨ì›ì´ ëŠ˜ì–´ë‚¬ë‹¤ê°€ ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¤„ì–´ë“œëŠ” ë‹¨ìˆœí•œ 2ì¸µ ì‹ ê²½ë§ì…ë‹ˆë‹¤.<br/>í™œì„±í™” í•¨ìˆ˜(Activation function)ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "$$\n",
        " \\text{FFNN}(x) = \\max(0, W_1x + b_1) W_2 +b_2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOLOYafDBGge"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    \"\"\"\n",
        "    FFNNì„ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "    Args:\n",
        "        d_model : ëª¨ë¸ì˜ ì°¨ì›ì…ë‹ˆë‹¤.\n",
        "        dff : ì€ë‹‰ì¸µì˜ ì°¨ì› ìˆ˜ì…ë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 2048ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFup_5pDupE"
      },
      "source": [
        "### Masked Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_kn1-AYR18G"
      },
      "source": [
        "<img width=\"300\" alt=\"Masked_Self-Attention_in_structure\" src=\"https://user-images.githubusercontent.com/45377884/112808936-78322f80-90b4-11eb-9315-22cd9caad41d.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNdrsLrER3w2"
      },
      "source": [
        "**<font color=\"ff6f61\">Masked Self-Attention</font>**ì€ ë””ì½”ë” ë¸”ë¡ì—ì„œ ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜í•œ Self-Attentionì…ë‹ˆë‹¤.<br/>\n",
        "ë””ì½”ë”ëŠ” Auto-Regressive(ì™¼ìª½ ë‹¨ì–´ë¥¼ ë³´ê³  ì˜¤ë¥¸ìª½ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡)í•˜ê²Œ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— íƒ€ê¹ƒ ë‹¨ì–´ ì´í›„ ë‹¨ì–´ë¥¼ ë³´ì§€ ì•Šê³  ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.<br/>\n",
        "ë”°ë¼ì„œ íƒ€ê¹ƒ ë‹¨ì–´ ë’¤ì— ìœ„ì¹˜í•œ ë‹¨ì–´ëŠ” Self-Attentionì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ **ë§ˆìŠ¤í‚¹(masking)**ì„ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8-UwtUcR5WO"
      },
      "source": [
        "<img width=\"500\" alt=\"Masked_Self-Attention_ex\" src=\"http://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33J9FxItR70W"
      },
      "source": [
        "***Self-Attention (without Masking) vs Masked Self-Attention***\n",
        "\n",
        "<img width=\"500\" alt=\"Masked_Self-Attention_ex2\" src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJfqdxHSAY3"
      },
      "source": [
        "Self-Attention ë©”ì»¤ë‹ˆì¦˜ì€ ì¿¼ë¦¬ í–‰ë ¬(Q)ì™€ í‚¤ í–‰ë ¬(K)ì˜ ë‚´ì í•©ë‹ˆë‹¤.<br/>\n",
        "ê²°ê³¼ë¡œ ë‚˜ì˜¨ í–‰ë ¬ì„ ì°¨ì›ì˜ ì œê³±ê·¼ $\\sqrt{d_k}$ ë¡œ ë‚˜ëˆ„ì–´ ì¤€ ë‹¤ìŒ,<br/> \n",
        "Softmaxë¥¼ ì·¨í•´ì£¼ê³  ë°¸ë¥˜ í–‰ë ¬(V)ê³¼ ë‚´ì í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
        "\n",
        "**Masked Self-Attention** ì—ì„œëŠ” Softmaxë¥¼ ì·¨í•´ì£¼ê¸° ì „, ê°€ë ¤ì£¼ê³ ì í•˜ëŠ” ìš”ì†Œì—ë§Œ $-\\infty$ ì— í•´ë‹¹í•˜ëŠ” ë§¤ìš° ì‘ì€ ìˆ˜ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.<br/>\n",
        "ì•„ë˜ ì½”ë“œ ì˜ˆì‹œì—ì„œëŠ” -10ì–µ(=`-1e9`)ì„ ë”í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.<br/>\n",
        "ì´ ê³¼ì •ì„ **ë§ˆìŠ¤í‚¹(Masking)**ì´ë¼ê³  í•©ë‹ˆë‹¤.<br/>\n",
        "ë§ˆìŠ¤í‚¹ëœ ê°’ì€ Softmaxë¥¼ ì·¨í•´ ì£¼ì—ˆì„ ë•Œ 0ì´ ë‚˜ì˜¤ë¯€ë¡œ Value ê³„ì‚°ì— ë°˜ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbTQf8oxzth-"
      },
      "source": [
        "<img width=\"600\" alt=\"masked_1\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-mask.png\">\n",
        "\n",
        "<img width=\"600\" alt=\"masked_2\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98r-D1WjSH3w"
      },
      "source": [
        "ìœ„ì—ì„œ ë“±ì¥í–ˆë˜ Self-Attentionì„ êµ¬í˜„ ì½”ë“œì—ì„œ `mask` ì™€ ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ë‹¤ì‹œ ë³´ë„ë¡ í•©ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03EldudrJgAD"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Attention ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "    q, k, v ì˜ leading dimensionì€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    k, vì˜ penultimate dimensionì´ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤, i.e.: seq_len_k = seq_len_v.\n",
        "\n",
        "    MaskëŠ” íƒ€ì…(padding or look ahead)ì— ë”°ë¼ ë‹¤ë¥¸ ì°¨ì›ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    ë§ì…ˆì‹œì—ëŠ” ë¸Œë¡œë“œìºìŠ¤íŒ… ë  ìˆ˜ ìˆì–´ì•¼í•©ë‹ˆë‹¤.\n",
        "    \n",
        "    Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "        \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "    \n",
        "    # matmul_qk(ì¿¼ë¦¬ì™€ í‚¤ì˜ ë‚´ì )ì„ dkì˜ ì œê³±ê·¼ìœ¼ë¡œ scaling í•©ë‹ˆë‹¤.\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    \"\"\"\n",
        "    maskê°€ ìˆì„ ê²½ìš° maskingëœ ìë¦¬(mask=1)ì—ëŠ” (-inf)ì— í•´ë‹¹í•˜ëŠ” ì ˆëŒ“ê°’ì´ í° ìŒìˆ˜ -1e9(=-10ì–µ)ì„ ë”í•´ì¤ë‹ˆë‹¤.\n",
        "    ê·¸ ê°’ì— softmaxë¥¼ ì·¨í•´ì£¼ë©´ ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ì´ ë‚˜ì˜µë‹ˆë‹¤. ê·¸ ë‹¤ìŒ value ê³„ì‚°ì‹œì— ë°˜ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "\n",
        "    # ë§ˆìŠ¤í‚¹ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) í•¨ìˆ˜ë¥¼ í†µí•´ì„œ attention weight ë¥¼ êµ¬í•´ë´…ì‹œë‹¤.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OSVcyskH543"
      },
      "source": [
        "### Encoder-Decoder Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUGAg5ikSb_F"
      },
      "source": [
        "<img width=\"300\" alt=\"Encoder-Decoder_Attention\" src=\"https://user-images.githubusercontent.com/45377884/112809435-f8f12b80-90b4-11eb-96e1-3b0f7c530659.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wepMQd4oSdkE"
      },
      "source": [
        "ë””ì½”ë”ì—ì„œ Masked Self-Attention ì¸µì„ ì§€ë‚œ ë²¡í„°ëŠ” **<font color=\"ff6f61\">Encoder-Decoder Attention</font>** ì¸µìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.<br/>\n",
        "ì¢‹ì€ ë²ˆì—­ì„ ìœ„í•´ì„œëŠ” **ë²ˆì—­í•  ë¬¸ì¥ê³¼ ë²ˆì—­ëœ ë¬¸ì¥ ê°„ì˜ ê´€ê³„** ì—­ì‹œ ì¤‘ìš”í•©ë‹ˆë‹¤.<br/>\n",
        "ë²ˆì—­í•  ë¬¸ì¥ê³¼ ë²ˆì—­ë˜ëŠ” ë¬¸ì¥ì˜ ì •ë³´ ê´€ê³„ë¥¼ ì—®ì–´ì£¼ëŠ” ë¶€ë¶„ì´ ë°”ë¡œ ì´ ë¶€ë¶„ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ì¸µì—ì„œëŠ” **ë””ì½”ë” ë¸”ë¡ì˜** Masked Self-Attentionìœ¼ë¡œë¶€í„° ì¶œë ¥ëœ ë²¡í„°ë¥¼ **ì¿¼ë¦¬(Q)** ë²¡í„°ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "**í‚¤(K)ì™€ ë°¸ë¥˜(V)** ë²¡í„°ëŠ” ìµœìƒìœ„(=6ë²ˆì§¸) ì¸ì½”ë” ë¸”ë¡ì—ì„œ ì‚¬ìš©í–ˆë˜ ê°’ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.<br/>\n",
        "**Encoder-Decoder Attention** ì¸µì˜ ê³„ì‚° ê³¼ì •ì€ Self-Attention í–ˆë˜ ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ëŠ” **Encoder-Decoder Attention** ê°€ ì§„í–‰ë˜ëŠ” ìˆœì„œë¥¼ ë‚˜íƒ€ë‚¸ ì´ë¯¸ì§€ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kawL4lmnSh0v"
      },
      "source": [
        "<img width=\"700\" alt=\"Encoder-Decoder_Attention_gif\" src=\"http://jalammar.github.io/images/t/transformer_decoding_1.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOn_vpeyRyyT"
      },
      "source": [
        "### Linear & Softmax Layer\n",
        "\n",
        "ì´ì œ ëì…ë‹ˆë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYcth4VySk4T"
      },
      "source": [
        "<img width=\"300\" alt=\"Linear_Softmax\" src=\"https://user-images.githubusercontent.com/45377884/112815762-994a4e80-90bb-11eb-9a57-a8be65c1a30b.png\">\n",
        "\n",
        "ë””ì½”ë”ì˜ ìµœìƒì¸µì„ í†µê³¼í•œ ë²¡í„°ë“¤ì€ Linear ì¸µì„ ì§€ë‚œ í›„ Softmaxë¥¼ í†µí•´ ì˜ˆì¸¡í•  ë‹¨ì–´ì˜ í™•ë¥ ì„ êµ¬í•˜ê²Œ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBx1WzlmRwcM"
      },
      "source": [
        "### ì½”ë“œ ì‹¤ìŠµ 1\n",
        "\n",
        "(ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ì›í™œí•œ ì‹¤ìŠµì„ ìœ„í•´ì„œ epoch ìˆ˜ë¥¼ ì¤„ì´ê³  í•™ìŠµí•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjPRXOJtlNVz"
      },
      "source": [
        "í•„ìš”í•œ ëª¨ë“ˆì„ import í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHWGjRnAKuG"
      },
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_6LDnLlQRn"
      },
      "source": [
        "í•™ìŠµíŒ” ë§ë­‰ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.<br/>\n",
        "ì•„ë˜ ì½”ë“œì—ì„œëŠ” ìŠ¤í˜ì¸ì–´ - ì˜ì–´ ë§ë­‰ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUsBQJDAALsO",
        "outputId": "e49e8cc4-fb8f-4098-da88-53b3a2d390c1"
      },
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wksDH89elfgW"
      },
      "source": [
        "í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì ì ˆíˆ ì „ì²˜ë¦¬ë¥¼ í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGcHrv5CATZl"
      },
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28DDN1TaleuC"
      },
      "source": [
        "ì „ì²˜ë¦¬ê°€ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. <br/>\n",
        "ë²ˆì—­ë˜ëŠ” ë¬¸ì¥ì˜ ì•ì—ëŠ” [start] í† í°ì„ ìœ„ì¹˜ì‹œí‚¤ê³  ë’¤ì—ëŠ” [end] í† í°ì„ ìœ„ì¹˜ì‹œí‚µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbd1UkphAWU9",
        "outputId": "0f4c8458-fd54-4721-c56c-54461dcc2216"
      },
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('You deserve a medal.', '[start] Te mereces una medalla. [end]')\n",
            "(\"Tom doesn't owe me anything.\", '[start] Tom no me debe nada. [end]')\n",
            "('I asked the boy to throw the ball back.', '[start] Le pedÃ­ al niÃ±o que me devolviera la pelota. [end]')\n",
            "('I drank from the tap.', '[start] He bebido del grifo. [end]')\n",
            "('Tom paid way too much for that old car.', '[start] Tom pagÃ³ demasiado por este carro viejo. [end]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knX0pkeqmRW9"
      },
      "source": [
        "ë°ì´í„°ì…‹ì„ split í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DX8hVKKAXwe",
        "outputId": "b1139086-ddf0-4934-a927-5986225db5f5"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqz8Noz4AZln"
      },
      "source": [
        "strip_chars = string.punctuation + \"Â¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKfCIzsxAbri"
      },
      "source": [
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELNse2GiAdf4",
        "outputId": "ab762b36-f635-4137-f896-bc34a92bf23d"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aVtnel1AfUG"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KCsWiekAg72"
      },
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxzP6SjtAk9Y",
        "outputId": "c6e8e32a-bb0d-46ea-f6e9-aafef2a91473"
      },
      "source": [
        "epochs = 30  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding (Positiona (None, None, 256)    3845120     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, None, 15000)  12959640    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 190s 143ms/step - loss: 0.9396 - accuracy: 0.6980 - val_loss: 1.0049 - val_accuracy: 0.6563\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.9190 - accuracy: 0.7064 - val_loss: 1.0081 - val_accuracy: 0.6586\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.9073 - accuracy: 0.7113 - val_loss: 1.0071 - val_accuracy: 0.6586\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 186s 143ms/step - loss: 0.8950 - accuracy: 0.7165 - val_loss: 1.0124 - val_accuracy: 0.6615\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 186s 143ms/step - loss: 0.8838 - accuracy: 0.7215 - val_loss: 1.0151 - val_accuracy: 0.6579\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8734 - accuracy: 0.7259 - val_loss: 1.0134 - val_accuracy: 0.6610\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8634 - accuracy: 0.7296 - val_loss: 1.0161 - val_accuracy: 0.6626\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8523 - accuracy: 0.7339 - val_loss: 1.0164 - val_accuracy: 0.6641\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8434 - accuracy: 0.7374 - val_loss: 1.0281 - val_accuracy: 0.6624\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8341 - accuracy: 0.7410 - val_loss: 1.0312 - val_accuracy: 0.6616\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 184s 141ms/step - loss: 0.8244 - accuracy: 0.7445 - val_loss: 1.0317 - val_accuracy: 0.6651\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8171 - accuracy: 0.7476 - val_loss: 1.0406 - val_accuracy: 0.6653\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 185s 142ms/step - loss: 0.8091 - accuracy: 0.7504 - val_loss: 1.0365 - val_accuracy: 0.6632\n",
            "Epoch 14/30\n",
            " 187/1302 [===>..........................] - ETA: 2:27 - loss: 0.7954 - accuracy: 0.7554"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8oTr__gAmjM"
      },
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXmedJR1AG1F"
      },
      "source": [
        "### ì½”ë“œ ì‹¤ìŠµ 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_-WgBYX5-4K"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX26HWbe6Cph"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_VY_WXJ6FdH"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvOXtcK76JV8"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py_M438h6OPe"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y38uKp96TQf"
      },
      "source": [
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPgdtKE03lGX"
      },
      "source": [
        "## ğŸ§  Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t22B1RW63zTq"
      },
      "source": [
        "- Attentionì˜ ì¥ì ì— ëŒ€í•´ì„œ ìƒê°í•˜ê³  ì„¤ëª…í•´ë´…ë‹ˆë‹¤.\n",
        "\n",
        "    - RNN ëª¨ë¸ì˜ ë‹¨ì  2ê°€ì§€\n",
        "    - ì¥ê¸° ì˜ì¡´ì„±(Long-term dependency)\n",
        "    - Attentionì˜ ì¥ì "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMlh3bET4NRZ"
      },
      "source": [
        "- Transformerì˜ ì¥ì ê³¼ êµ¬ì¡°ì— ëŒ€í•´ì„œ ìƒê°í•˜ê³  ì„¤ëª…í•´ë´…ë‹ˆë‹¤.\n",
        "    - \"Attention is All You Need\" (ì™œ ë…¼ë¬¸ ì œëª©ì„ ì´ë ‡ê²Œ ì§€ì—ˆì„ì§€ì— ëŒ€í•´ì„œ ìƒê°í•´ë´…ì‹œë‹¤)\n",
        "    - Positional Encoding\n",
        "    - Self-Attention\n",
        "    - Masked Self-Attention\n",
        "    - Encoder-Decoder Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XOZcD7x6SR2"
      },
      "source": [
        "- GPT & BERT\n",
        "    - ì‚¬ì „ í•™ìŠµ ì–¸ì–´ ëª¨ë¸(Pretrained Language Model), ì „ì´ í•™ìŠµ(Transfer Learning)\n",
        "        - ì‚¬ì „ í•™ìŠµ(Pre-training)\n",
        "        - Fine-tuning\n",
        "    - GPTì˜ êµ¬ì¡°\n",
        "    - BERTì˜ êµ¬ì¡°\n",
        "        - MLM(Masked Langauge Model)\n",
        "        - NSP(Next Sentence Prediction)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQlhRzWQsQN1"
      },
      "source": [
        "### ğŸ” References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHL_Cr3IA0IL"
      },
      "source": [
        "- íŠ¸ëœìŠ¤í¬ë¨¸ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ìì„¸í•˜ê²Œ ì•Œê³  ì‹¶ë‹¤ë©´\n",
        "    - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "    - [ë²ˆì—­](https://nlpinkorean.github.io/illustrated-transformer/)\n",
        "    - [Paper](https://arxiv.org/pdf/1706.03762.pdf) (Attention is All You Need)\n",
        "\n",
        "- GPTì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ ì•Œê³  ì‹¶ë‹¤ë©´\n",
        "    - [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) (Visualizing Transformer Language Models)\n",
        "    - [Paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf) (Improving Language Understanding by Generative Pre-Training)\n",
        "\n",
        "- BERTì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ ì•Œê³  ì‹¶ë‹¤ë©´\n",
        "    - [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/) (How NLP Cracked Transfer Learning)\n",
        "    - [ë²ˆì—­](https://nlpinkorean.github.io/illustrated-bert/)\n",
        "    - [Paper](https://arxiv.org/pdf/1810.04805.pdf) (Pre-training of Deep Bidirectional Transformers for\n",
        "Language Understanding)"
      ]
    }
  ]
}
