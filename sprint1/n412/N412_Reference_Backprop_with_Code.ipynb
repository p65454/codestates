{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N412_Reference_Backprop_with_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELV1ffGLL6xY"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *AIB / SECTION 4 / SPRINT 1 / NOTE 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO0YR1yO0N8"
      },
      "source": [
        "## 역전파(Backpropagation, BP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecLK2exeO85-"
      },
      "source": [
        "<img src=\"https://i.imgur.com/Y5XVbrp.png\" title=\"https://becominghuman.ai/understanding-the-structure-of-neural-networks-1fa5bd17fef0\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00yjbAa230aE"
      },
      "source": [
        "### 예제를 통해서 이해해보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6VaoSzKO-z7"
      },
      "source": [
        "**`공부시간`** 과 **`수면시간`** 을 특성(Feature)으로 하고 시험 점수를 레이블로 하는 회귀 예제를 풀어봅시다.<br/>\n",
        "데이터에서 특성과 레이블은 아래와 같은 선형 관계를 이루고 있다고 가정하겠습니다.<br/>\n",
        "아래 식에서 $x_1$ 은 **`공부시간`**을 나타내고 $x_2$ 는 **`수면시간`**을 나타낸다고 해보겠습니다.\n",
        "\n",
        "$y = 5x_1 + 2x_2 + 40$\n",
        "\n",
        "그리고 신경망이 위 관계를 알아서 잘 찾아낼 수 있는지를 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ26yTfL3yRw"
      },
      "source": [
        "1. **필요한 패키지를 `import` 하고 랜덤 시드(random seed)를 고정합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuW58m1LO4e-"
      },
      "source": [
        "# 해당 코드는 선형함수를 예측하는 예제입니다. \n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(812)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpIaJT6_4Kqx"
      },
      "source": [
        "2. **임의의 특성 데이터로부터 관계를 만족하는 레이블을 도출한 뒤에 데이터셋을 만듭니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd81KjHQ4LH4"
      },
      "source": [
        "# [공부시간, 수면시간]\n",
        "X = np.array(([8,8],\n",
        "              [2,5],\n",
        "              [7,6]), dtype=float)\n",
        "\n",
        "# 선형 관계를 바탕으로 시험 점수 레이블을 생성합니다.\n",
        "y = X[:,0]*5 + X[:,1]*2\n",
        "y = y.reshape(3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjNcGpJTE3yS"
      },
      "source": [
        "3. **특성을 정규화(Normalization) 합니다.**\n",
        "\n",
        "각 특성 및 레이블의 최댓값으로 나누어 주어 0~1 사이의 값으로 만들어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JntJxcVZE5JP",
        "outputId": "7c37aec9-fb11-4b7e-f77a-b240a8769147"
      },
      "source": [
        "X = X / np.amax(X, axis=0)\n",
        "y = y / np.amax(y, axis=0)\n",
        "\n",
        "print(\"공부시간, 수면시간 \\n\", X)\n",
        "print(\"시험점수 \\n\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "공부시간, 수면시간 \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "시험점수 \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LdNeb6nPCGP"
      },
      "source": [
        "4. **신경망을 구축합니다.**\n",
        "\n",
        "> ❗️ ***클래스 개념이 익숙하지 않을 수 있습니다.<br/>\n",
        "클래스 자체의 의미보다는 주석을 따라 코드를 한줄 한줄 따라가 보도록 합시다.***\n",
        "\n",
        "`NeuralNetwork` 클래스 내 `__init__` 메소드(함수)에서 신경망을 구축합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmDZfDC1LrHh"
      },
      "source": [
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    신경망(Neural network)를 정의하는 클래스(Class) 선언\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        신경망의 구조를 결정합니다.\n",
        "\n",
        "        inputs : 입력층 노드 수\n",
        "        hiddenNodes : 은닉층 노드 수\n",
        "        outputNodes : 출력층 노드 수\n",
        "        w1, w2 : 은닉층(layer 1), 출력층(layer 2)의 가중치\n",
        "        \"\"\"\n",
        "        \n",
        "        self.inputs = 2\n",
        "        self.hiddenNodes = 3\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        # 가중치를 초기화 합니다.\n",
        "        # layer 1 가중치 shape : 2x3\n",
        "        self.w1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
        "        \n",
        "        # layer 2 가중치 shape : 3x1\n",
        "        self.w2 = np.random.randn(self.hiddenNodes, self.outputNodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g48qC1Xy7Q-R"
      },
      "source": [
        "> ❓ ***왜 입력 노드의 수는 2로 해주었을까요?***<br/>\n",
        "❓ ***왜 출력 노드의 수는 1로 해주었을까요?***<br/>\n",
        "❓ ***왜 `layer 1, layer 2`의 가중치 행렬 shape 은 각각 2X3, 3X1 로 설정하였을까요?***\n",
        "\n",
        "> ❗️ ***신경망에서 <font color=\"ff6f61\">가중치 행렬의 `shape`은 굉장히 중요하면서도 헷갈리는 부분</font>입니다.<br/>\n",
        "단번에 완벽히 이해할 수는 없겠지만 반복하여 보면서 익숙해져 봅시다!***\n",
        "\n",
        "실제로 가중치가 어떻게 생성되었는지 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vg259zrPGY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d437bad3-8128-4351-8ed3-94f97d6e7d16"
      },
      "source": [
        "# 정의된 클래스를 사용해보고, 해당 가중치를 디스플레이 하는 코드입니다. \n",
        "nn = NeuralNetwork()\n",
        "\n",
        "print(\"Layer 1 가중치: \\n\", nn.w1)\n",
        "print(\"Layer 2 가중치: \\n\", nn.w2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 가중치: \n",
            " [[ 2.48783189  0.11697987 -1.97118428]\n",
            " [-0.48325593 -1.50361209  0.57515126]]\n",
            "Layer 2 가중치: \n",
            " [[-0.20672583]\n",
            " [ 0.41271104]\n",
            " [-0.57757999]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvxnl3IaAQAQ"
      },
      "source": [
        "- **Review : 행렬의 곱셈 연산**\n",
        "\n",
        "$A_{l \\times m}, B_{m \\times n}$ 두 행렬을 곱할 때 $\\Rightarrow (AB)_{l \\times n}$<br/>\n",
        "결과값으로 나오는 행렬의 shape은 $\\big($<font color='red'>$l$</font> $\\times$ <font color='blue'>$m$</font>$\\big)$ $\\cdot$ $\\big($ <font color='blue'>$m$</font> $\\times$ <font color='green'>$n$</font> $\\big)$ = <font color='red'>$l$</font> $\\times$ <font color='green'>$n$</font> 행렬의 형태로 연산이 됩니다.\n",
        "\n",
        "아래 그림을 참조하여 행렬의 곱셈 연산에 대한 내용을 다시 떠올려봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyCt_jelPKB9"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/Matrix_multiplication_qtl1.svg\" width=\"450\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIZ-3Q2qDaUO"
      },
      "source": [
        "5. **순전파 기능을 추가로 구현합니다.**\n",
        "\n",
        "**기존 `NeuralNetwork` 클래스에 순전파 기능을 추가**하여 봅시다.<br/>\n",
        "순전파에 필요한 활성화 함수(sigmoid)를 구현한 뒤에 가중합(weighted sum)부분과 활성화 함수가 적용되는 부분을 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtoouVz8PKmM"
      },
      "source": [
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        신경망의 구조를 결정합니다.\n",
        "\n",
        "        inputs : 입력층 노드 수\n",
        "        hiddenNodes : 은닉층 노드 수\n",
        "        outputNodes : 출력층 노드 수\n",
        "        w1, w2 : layer 1, layer 2의 가중치\n",
        "        \"\"\"\n",
        "        self.inputs = 2\n",
        "        self.hiddenNodes = 3\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        # 가중치를 초기화 합니다.\n",
        "        # layer 1 가중치 shape : 2x3\n",
        "        self.w1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
        "        \n",
        "        # layer 2 가중치 shape : 3x1\n",
        "        self.w2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        \"\"\"\n",
        "        활성화 함수인 시그모이드 함수를 정의합니다.\n",
        "        s : 활성화 함수에 입력되는 값(=가중합)\n",
        "        \"\"\"\n",
        "        return 1 / (1+np.exp(-s))\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        \"\"\"\n",
        "        순전파를 구현합니다.\n",
        "        입력 신호를 받아 출력층의 결과를 반환합니다.\n",
        "        \n",
        "        hidden_sum : 은닉층(layer 1)에서의 가중합(weighted sum)\n",
        "        activated_hidden : 은닉층(layer 1) 활성화 함수의 함숫값\n",
        "        output_sum : 출력층(layer 2)에서의 가중합(weighted sum)\n",
        "        activated_output : 출력층(layer 2) 활성화 함수의 함숫값\n",
        "        \"\"\"\n",
        "        \n",
        "        self.hidden_sum = np.dot(X, self.w1)\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.w2)\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtV91UhbI37E"
      },
      "source": [
        "6. **순전파를 거쳐 출력되는 값을 알아봅시다.**\n",
        "\n",
        "입력한 데이터가 신경망 순전파를 거쳐 어떤 출력값을 내는 지 확인해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAQenxirPS8A"
      },
      "source": [
        "# 선언한 클래스를 불러와서 할당합니다.\n",
        "nn = NeuralNetwork()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW2YtQCqJUao",
        "outputId": "66cc63ee-e988-413b-8d08-cdcb1a1d7174"
      },
      "source": [
        "# 첫 번째 데이터를 출력합니다.\n",
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXZjYC74JXRR",
        "outputId": "a4db6bc2-5cc7-4f65-c8b3-8c9546e35a10"
      },
      "source": [
        "# 첫 번째 데이터를 입력한 뒤 신경망이 출력하는 값을 살펴보겠습니다.\n",
        "output = nn.feed_forward(X[0])\n",
        "print(\"예측값: \", output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측값:  [0.21945787]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gTPu7dEPUYf"
      },
      "source": [
        "7. **손실(Error,Loss)과 비용(Cost) 계산**\n",
        "\n",
        "1개의 데이터에 대해 실제 타겟 레이블과 출력값을 비교하여 손실(`error`)을 구해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAawfPI5PV-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6290b888-bf2a-4a1b-a715-6dc15d08d98c"
      },
      "source": [
        "# 실제 타겟 레이블과 출력값을 비교하여 손실(error)을 구합니다.\n",
        "error = y[0] - output\n",
        "error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.78054213])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcRHiMlMMoch"
      },
      "source": [
        "이 과정을 모든 데이터에 적용하여 비용(Cost)를 구해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY7akPN6MvPy",
        "outputId": "b4434595-8637-4945-bb9e-949c82512c9e"
      },
      "source": [
        "# 모든 데이터를 예측해보고 에러값을 계산해 보겠습니다.\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StYYK5D8M12y",
        "outputId": "19c6c63e-72ce-471a-c1d1-c579f9728b6c"
      },
      "source": [
        "output_all = nn.feed_forward(X)\n",
        "print(output_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.21945787]\n",
            " [0.34573206]\n",
            " [0.23788921]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv3vM_ckM9yS",
        "outputId": "1504e486-98e4-4cf2-b76c-7b5d9bdcd544"
      },
      "source": [
        "error_all = y - output_all\n",
        "print(error_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.78054213]\n",
            " [0.0114108 ]\n",
            " [0.6013965 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7bWbvMrND2o"
      },
      "source": [
        "#### 결과값 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZ54d0QNMkj"
      },
      "source": [
        "1. 각각의 **에러가 작어야 하는데 너무 크게 나왔습니다.** 에러가 크게 나오는 이유는 무엇일까요?\n",
        "    - 에러가 높게 나오는 이유는 예측값이 **정확하지 않기(여기서는 너무 작게 나오기) 때문**입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoD92VIMPBsU"
      },
      "source": [
        "2. 그렇다면 **예측값이 작게 나오는 이유**는 무엇일까요?\n",
        "    - 임의로 지정하였던 두 번째 층의 가중치 값(`w2`)이 작거나\n",
        "    - 첫 번째 층의 출력값(`activated_hidden`)이 작기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1khxbqguPrsa"
      },
      "source": [
        "3. **첫 번째 층의 출력값(`activated_hidden`)이 작은 이유**는 무엇일까요?\n",
        "- 입력 데이터(`X`)는 변하지 않는 값이므로 첫 번째 층의 가중치 값(`w1`)이 작기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZJf6lWFQZUj"
      },
      "source": [
        "**1, 2, 3**을 고려했을 때 예측값을 증가시키기 위한 방법은 **첫 번째 층과 두 번째 층의 가중치(`w1, w2`)를 증가시키는 것**뿐입니다.\n",
        "\n",
        "이렇게 각 층마다 가중치가 있을텐데 에러를 최소화하기 위해서 어떤 가중치를 얼마나 올려주어야 할까요?<br/>\n",
        "일단 각 층의 가중치를 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAJQ90D7Pb-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026a624c-ee78-4455-f6d6-2d1be825012b"
      },
      "source": [
        "# 각각의 변수(가중치)를 디스플레이 하기 위한 코드입니다.\n",
        "attributes = ['w1', 'hidden_sum', 'activated_hidden', 'w2', 'activated_output']\n",
        "\n",
        "for i in attributes:\n",
        "    if i[:2] != '__':\n",
        "        print(i+'\\n', getattr(nn,i), '\\n'+'---'*3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1\n",
            " [[-1.75351135  1.23279898  0.24464757]\n",
            " [-0.06568225  0.30190098  0.79723428]] \n",
            "---------\n",
            "hidden_sum\n",
            " [[-1.8191936   1.53469996  1.04188185]\n",
            " [-0.47942924  0.49688786  0.55943332]\n",
            " [-1.58358412  1.30512484  0.81199233]] \n",
            "---------\n",
            "activated_hidden\n",
            " [[0.13953066 0.82269293 0.73921295]\n",
            " [0.38238691 0.62172769 0.63632141]\n",
            " [0.17028848 0.78669622 0.6925339 ]] \n",
            "---------\n",
            "w2\n",
            " [[ 1.23073545]\n",
            " [-1.52187331]\n",
            " [-0.25502715]] \n",
            "---------\n",
            "activated_output\n",
            " [[0.21945787]\n",
            " [0.34573206]\n",
            " [0.23788921]] \n",
            "---------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3MHHUG4RHQh"
      },
      "source": [
        "#### 손실(Error)을 줄이기 위해서는 어떻게 해야 할까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzI9r1nNPcbZ"
      },
      "source": [
        "아래 그림을 보면 알 수 있듯, **비용 함수 $J$ 의 경사(Gradient)가 작아지는 방향으로 업데이트** 하면 손실 함수의 값을 줄일 수 있습니다.<br/>\n",
        "매 Iteration 마다 **<font color=\"ff6f61\">해당 가중치에서의 비용 함수의 도함수(=비용 함수를 미분한 함수)를 계산</font>하여** 경사가 작아질 수 있도록 가중치를 변경합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaobXDMGPi5b"
      },
      "source": [
        "<img src=\"https://i.imgur.com/ehYYRtw.png\" alt=\"Gradient Descent in 1D\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uf9AGwaPioi"
      },
      "source": [
        "위에서 설계한 신경망은 총 9개의 가중치를 가지고 있습니다. 첫 번째 층에는 6개(`w1`), 두 번째 층에는 3개(`w2`)가 있죠.<br/>\n",
        "그렇기 때문에 해당 신경망의 비용 함수는 9차원 공간상의 함수$(J)$가 되겠습니다.<br/>\n",
        "비용 함수의 경사가 줄어드는 방향으로 가중치를 갱신해나가면 되겠죠?\n",
        "\n",
        "비용 함수 $J$를 수식으로 나타내 보겠습니다.\n",
        "\n",
        "$$\n",
        "J(\\theta) = J(\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5, \\theta_6, \\theta_7, \\theta_8, \\theta_9)\n",
        "$$\n",
        "\n",
        "아래는 비용 함수를 가중치 2개에 대해서만 단순화시켜 나타낸 그림입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpHICzw8Pilo"
      },
      "source": [
        "<img src=\"https://i.imgur.com/yZA6RUJ.png\" alt=\"Gradient descent algorithm direction Equation 1. (Image courtesy of Andrew Ng)\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_PSaZJ9PqNt"
      },
      "source": [
        "- **볼록/오목 함수(Convex/Concave function)와 지역 최적점(Local Optima)**\n",
        "\n",
        "경사 하강법을 통해 최저점을 찾는 메커니즘은 볼록(Convex) 함수에서만 잘 동작합니다.<br/>\n",
        "하지만 실제 손실 함수는 위 그림처럼 볼록 함수와 오목 함수가 부분부분 섞여있는 형태인데요.<br/>\n",
        "그렇기 때문에 전역 최적점(Global Optima)를 찾지 못하고 **지역 최적점(Local Optima)에 빠질 수 있습니다.**\n",
        "\n",
        "지역 최적점에 빠지게 되는 문제를 방지하기 위한 여러가지 방법이 있는데요.<br/>\n",
        "이런 알고리즘에 대해서는 잠시 후에 알아보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSkwMFVqBLgZ"
      },
      "source": [
        "> ❗️ ***볼록 함수(Convex function)와 오목 함수(Concave function)의 수학적인 의미보다는 각 함수가 대략 어떤 형태인지 알아봅시다.<br/>\n",
        "각 함수의 형태로부터 볼록 함수에서는 왜 경사 하강법이 잘 되고, 오목 함수에서는 안 될지 상상해봅시다.***\n",
        "\n",
        "> 🔍 ***참조 링크 : [Convex(볼록) & Concave(오목) 알아보기](https://blog.naver.com/sw4r/221148661854) <br/>\n",
        "추천 검색어 : Convex Optimization, Gradient Descent***\n",
        "\n",
        "> ❗️ ***그리고 지역 최적점이란 무엇인지, 왜 지역 최적점에 빠지면 안좋은지를 알아봅시다.***\n",
        "\n",
        "> 🔍 ***추천 검색어 : Gradient Descent Local Optima***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CduJwgvPhr-"
      },
      "source": [
        "### 가중치 업데이트 : 역전파 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcP_MVZ7kaWy"
      },
      "source": [
        "6. **<font color=\"ff6f61\">역전파 기능을 구현</font>해봅시다.**\n",
        "\n",
        "> ❗️ ***역전파(***`backward`***) 함수 내 과정이 이해가 되지 않을 수 있습니다.<br/>\n",
        "아래에서 다시 설명이 될 예정이므로 당장 이해가 되지 않더라도 좋습니다.***\n",
        "\n",
        "> ❗️ ***아래 그림은 역전파 시 각 변수(`o_error`,`o_delta`,`z2_error`,`z2_delta`)의 이해를 돕기 위한 것으로 노드 수는 신경쓰지 않으셔도 됩니다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdvgnpemTgp9"
      },
      "source": [
        "<img src=\"https://i.imgur.com/brCdkz1.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GuCg1YaPjpu"
      },
      "source": [
        "# 음수 가중치를 가지는 활성화는 낮추고, 양수 가중치를 가지는 활성화는 높이고 싶습니다.\n",
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        신경망의 구조를 결정합니다.\n",
        "\n",
        "        inputs : 입력층 노드 수\n",
        "        hiddenNodes : 은닉층 노드 수\n",
        "        outputNodes : 출력층 노드 수\n",
        "        w1, w2 : layer 1, layer 2의 가중치\n",
        "        \"\"\"\n",
        "        self.inputs = 2\n",
        "        self.hiddenNodes = 3\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        # 가중치를 초기화 합니다.\n",
        "        # layer 1 가중치 shape : 2x3\n",
        "        self.w1 = np.random.randn(self.inputs,self.hiddenNodes)\n",
        "        \n",
        "        # layer 2 가중치 shape : 3x1\n",
        "        self.w2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        \"\"\"\n",
        "        활성화 함수인 시그모이드 함수를 정의합니다.\n",
        "        s : 순전파 과정에서 활성화 함수에 입력되는 값(=가중합)\n",
        "        \"\"\"\n",
        "        return 1 / (1+np.exp(-s))\n",
        "\n",
        "    def sigmoidPrime(self, s):\n",
        "        \"\"\"\n",
        "        활성화 함수(sigmoid)를 미분한 함수입니다.\n",
        "        s : 순전파 과정에서 활성화 함수에 입력되는 값(=가중합)\n",
        "        \"\"\"\n",
        "        sx = self.sigmoid(s)\n",
        "        return sx * (1-sx)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        \"\"\"\n",
        "        순전파를 구현합니다.\n",
        "        입력 신호를 받아 출력층의 결과를 반환합니다.\n",
        "        \n",
        "        hidden_sum : 은닉층(layer 1)에서의 가중합(weighted sum)\n",
        "        activated_hidden : 은닉층(layer 1) 활성화 함수의 함숫값\n",
        "        output_sum : 출력층(layer 2)에서의 가중합(weighted sum)\n",
        "        activated_output : 출력층(layer 2) 활성화 함수의 함숫값\n",
        "        \"\"\"\n",
        "        \n",
        "        self.hidden_sum = np.dot(X, self.w1)\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.w2)\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        \"\"\"\n",
        "        역전파를 구현합니다.\n",
        "        출력층에서 손실 값(Error)를 구한 뒤에 이를 각 가중치에 대해 미분한 값만큼 가중치를 수정합니다.\n",
        "\n",
        "        X : 입력 데이터(input)\n",
        "        y : 타겟값(target value)\n",
        "        o : 출력값(output)\n",
        "\n",
        "        o_error : 손실(Error) = 타겟값과 출력값의 차이\n",
        "        o_delta : 출력층 활성화 함수의 미분값\n",
        "        \"\"\"\n",
        "        \n",
        "        # o_error : 손실(Error)을 구합니다.\n",
        "        self.o_error = y - o \n",
        "        \n",
        "        # o_delta : 활성화 함수(시그모이드)의 도함수를 사용하여 출력층 활성화 함수 이전의 미분값을 구합니다.\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "        \n",
        "        # z2 error : 은닉층에서의 손실을 구합니다.\n",
        "        self.z2_error = self.o_delta.dot(self.w2.T)\n",
        "        \n",
        "        # z2 delta : 활성화 함수(시그모이드)의 도함수를 사용하여 은닉층 활성화 함수 이전의 미분값을 구합니다.\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.output_sum)\n",
        "\n",
        "        # w1, w2를 업데이트 합니다.\n",
        "        self.w1 += X.T.dot(self.z2_delta) # X * dE/dY * dY/dy(=Y(1-Y))\n",
        "        self.w2 += self.activated_hidden.T.dot(self.o_delta) # H1 * Y(1-Y) * (Y - o)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        \"\"\"\n",
        "        실제로 신경망 학습을 진행하는 코드입니다.\n",
        "        1번의 순전파-역전파, 즉 1 iteration 을 수행하는 함수입니다.\n",
        "        \n",
        "        X : 입력 데이터(input)\n",
        "        y : 타겟값(target value)\n",
        "        \"\"\"\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X,y,o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTx0-U7ynZmw"
      },
      "source": [
        "> ❗️ ***당장 아래 수식을 이해하지 못해도 좋습니다. 아래에서 수학을 통해 알아볼 것입니다.<br/>\n",
        "다만 나중에는 수식만 보고도 이해할 수 있어야겠죠?***\n",
        "\n",
        "- 수식을 통해 위 코드를 알아보겠습니다.\n",
        "\n",
        "    손실(Error) : $E$<br/>\n",
        "출력층 활성화 함수의 출력값(`activated_output`) : $A_O$<br/>\n",
        "출력층 활성화 함수의 입력값(=가중합, `output_sum`) : $S_O$<br/>\n",
        "은닉층 활성화 함수의 출력값(`activated_hidden`) : $A_H$<br/>\n",
        "은닉층 활성화 함수의 입력값(=가중합, `hidden_sum`) : $S_H$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfh-8J7XqVi1"
      },
      "source": [
        "- **$w_2$(=출력층과 은닉층 사이의 가중치) 에 대한 미분**\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial E}{\\partial w_2} &= \\frac{\\partial E}{\\partial A_O} \\cdot \\frac{\\partial A_O}{\\partial w_2}\\\\\n",
        "\\frac{\\partial E}{\\partial w_2} &= \\frac{\\partial E}{\\partial A_O} \\cdot \n",
        "\\frac{\\partial A_O}{\\partial S_O} \\cdot \\frac{\\partial S_O}{\\partial w_2}\n",
        "= \\frac{\\partial E}{\\partial A_O} \\cdot \n",
        "\\frac{\\partial A_O}{\\partial S_O} \\cdot A_H \\quad \\bigg( \\because A_H = \\frac{\\partial S_O}{\\partial w_2}\\bigg)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "- **$w_1$(=은닉층과 입력층 사이의 가중치) 에 대한 미분**\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial E}{\\partial w_1} &= \\frac{\\partial E}{\\partial A_H} \\cdot \\frac{\\partial A_H}{\\partial w_1}\\\\\n",
        "\\frac{\\partial E}{\\partial w_2} &= \\frac{\\partial E}{\\partial A_H} \\cdot \n",
        "\\frac{\\partial A_H}{\\partial S_H} \\cdot \\frac{\\partial S_H}{\\partial w_1}\n",
        "= \\frac{\\partial E}{\\partial A_H} \\cdot \n",
        "\\frac{\\partial A_H}{\\partial S_H} \\cdot X \\quad \\bigg( \\because X = \\frac{\\partial S_H}{\\partial w_1}\\bigg)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9r5CAlht1Sb"
      },
      "source": [
        "#### 역전파가 추가된 신경망 클래스 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evfTvj1U2Yqd"
      },
      "source": [
        "- **순전파 후 손실(Error) 계산**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUmwEIh3PqCD"
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "nn.train(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evjw1NK4Pp_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ba2c75-07fc-4b04-895d-b689911814dc"
      },
      "source": [
        "# 순전파 후 손실(Error)을 확인해보겠습니다.\n",
        "nn.o_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.70644301],\n",
              "       [0.10036169],\n",
              "       [0.56649547]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAbq8hWCPtn6"
      },
      "source": [
        "- **출력층의 경사(Gradient) 계산하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNWqlpnDVNUL"
      },
      "source": [
        "**에러(Error, `o_error`)**와 **출력층 활성화 함수를 미분한 함수(`sigmoidPrime`)**를 통해서 출력층의 경사(`o_delta`)를 구해보겠습니다.\n",
        "\n",
        "`self.o_delta = self.o_error * self.sigmoidPrime(self.output_sum)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfu_hl8CPvTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafa1e5d-de5b-4c0a-e90f-8f203c9397bf"
      },
      "source": [
        "# 순전파 시 출력층에서의 가중합\n",
        "nn.o_error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.70644301],\n",
              "       [0.10036169],\n",
              "       [0.56649547]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxVc1iycPxmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132de16b-5ccb-49f4-fee6-10934f000d2b"
      },
      "source": [
        "# 순전파 시 출력층에서의 가중합을 활성화 함수에 통과시킨 \n",
        "nn.sigmoidPrime(nn.o_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22123085],\n",
              "       [0.24937153],\n",
              "       [0.23096866]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjfaGwQPx7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a65542b-6523-4592-fb4a-ceaf91fb0c7d"
      },
      "source": [
        "# 출력층 활성화 함수 이전의 미분값을 구합니다.\n",
        "# o_delta = o_error * sigmoidPrime(o)\n",
        "nn.o_delta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17285985],\n",
              "       [0.02468133],\n",
              "       [0.13902149]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQyKhgPmP0cF"
      },
      "source": [
        "- **은닉층이 받는 손실(Error) 계산**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EljA-ddtVj7A"
      },
      "source": [
        "이전 단계에서 구했던 **출력층의 경사(`o_delta`)**와 **출력층의 가중치(`w2`)**를 통해서 은닉층이 받는 손실(`z2_error`)을 구해보겠습니다.\n",
        "\n",
        "`self.z2_error = self.o_delta.dot(self.w2.T)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ3pQbwrP0uP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d7a5bb-9ffb-4bbd-8b46-8b8d931fd945"
      },
      "source": [
        "nn.o_delta.dot(nn.w2.T)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.28194591, -0.23236088,  0.12295841],\n",
              "       [-0.04025689, -0.03317703,  0.01755629],\n",
              "       [-0.22675329, -0.18687483,  0.09888856]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaN7hHWmWDRY"
      },
      "source": [
        "> ❗️ ***해당 연산의 결과로 나오는 행렬의 shape이 왜 (3,3)일지에 대해서 생각해보고 토론해봅시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-GvmzdCWQgo"
      },
      "source": [
        "- **은닉층의 경사(Gradient) 계산하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em8eSpBqP4Jd"
      },
      "source": [
        "**은닉층 에러(`z2_error`)**와 **은닉층 활성화 함수를 미분한 함수(`sigmoidPrime`)**를 통해서 은닉층의 경사(`z2_delta`)를 구해보겠습니다.\n",
        "\n",
        "`self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNp-1OzqP5rT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c75bde-8a67-4622-e812-634fa1aa0222"
      },
      "source": [
        "nn.activated_hidden"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.43460965, 0.23985861, 0.42340322],\n",
              "       [0.37111212, 0.48687214, 0.52242758],\n",
              "       [0.48093658, 0.2520165 , 0.4192448 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfC1no3TP5pE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636e1755-0ecc-4fb2-8298-c2aedd4e5384"
      },
      "source": [
        "nn.z2_delta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06388859, -0.05136035,  0.020324  ],\n",
              "       [-0.00839476, -0.00674859,  0.00267051],\n",
              "       [-0.04915074, -0.03951252,  0.01563565]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAzRNK57P5mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b54e03b-f58e-48d9-a471-10bbaffe696b"
      },
      "source": [
        "X.T.shape == nn.w1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPB5af8rZW6d",
        "outputId": "dba642af-38c9-46ac-95a4-4fab8fefd87e"
      },
      "source": [
        "nn.z2_delta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06388859, -0.05136035,  0.020324  ],\n",
              "       [-0.00839476, -0.00674859,  0.00267051],\n",
              "       [-0.04915074, -0.03951252,  0.01563565]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8cQi2-XZeza"
      },
      "source": [
        "#### 경사 하강법(Gradient Descent)을 적용하여 가중치 업데이트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_q1Hh-BP_gX"
      },
      "source": [
        "- 은닉층 가중치(`w1`)를 업데이트 합니다.\n",
        "\n",
        "> ❗️ ***토론해봅시다.<br/>\n",
        "1) 왜 입력값(`X`)와 은닉층 기울기 값(`z2_delta`)을 곱해주는 것일까요?<br/>\n",
        "2) `X`는 왜 Transpose 해주어야 할까요?***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS66sQMBQAfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b6c0ff-3371-4447-9bce-47038a1b594a"
      },
      "source": [
        "X.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.   , 0.25 , 0.875],\n",
              "       [1.   , 0.625, 0.75 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zyPq4O3QAcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42df4cbd-c441-48d5-8adb-c952a8f21852"
      },
      "source": [
        "X.T.dot(nn.z2_delta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.10899418, -0.08762095,  0.03467281],\n",
              "       [-0.10599837, -0.0852126 ,  0.0337198 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVcrBcMIaa5f"
      },
      "source": [
        "- 출력층 가중치(`w2`)를 업데이트 합니다.\n",
        "\n",
        "> ❗️ ***토론해봅시다.<br/>\n",
        "1) 왜 첫 번째 층의 출력값(`activated_hidden`)과 출력층의 기울기(`o_delta`)를 곱해주는 것일까요?<br/>\n",
        "2) 출력층의 shape은 어떻게 3X1이 될까요?***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjt0nZL_QFGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b7d9e6-d1c8-4818-adda-fcc874709ed4"
      },
      "source": [
        "nn.activated_hidden.T.dot(nn.o_delta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15114662],\n",
              "       [0.08851428],\n",
              "       [0.14436766]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XndaS_3LQHyQ"
      },
      "source": [
        "### 신경망 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AboyJKFhbc2q"
      },
      "source": [
        "이제 순전파와 역전파를 정해진 횟수(iterations or epochs)만큼 반복하여 신경망을 학습시켜 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbCo1KQMQJok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1af69c2-c779-48c8-cb27-fb633495bf3f"
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "# 반복수(epochs or iterations)를 정합니다.\n",
        "iter = 10000\n",
        "\n",
        "# 지정한 반복수 만큼 반복합니다.\n",
        "for i in range(iter):\n",
        "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 == 0):\n",
        "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
        "        print('입력: \\n', X)\n",
        "        print('타겟출력: \\n', y)\n",
        "        print('예측: \\n', str(nn.feed_forward(X)))\n",
        "        print(\"에러: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
        "    nn.train(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------EPOCH 1---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.37768395]\n",
            " [0.40886333]\n",
            " [0.39684567]]\n",
            "에러: \n",
            " 0.19523515397548788\n",
            "+---------EPOCH 2---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.45734397]\n",
            " [0.47891087]\n",
            " [0.47278046]]\n",
            "에러: \n",
            " 0.14787637084266458\n",
            "+---------EPOCH 3---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.52094752]\n",
            " [0.53342462]\n",
            " [0.5325371 ]]\n",
            "에러: \n",
            " 0.11822041803732024\n",
            "+---------EPOCH 4---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.57039035]\n",
            " [0.57514949]\n",
            " [0.57861702]]\n",
            "에러: \n",
            " 0.10001317226025601\n",
            "+---------EPOCH 5---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.60862211]\n",
            " [0.60709769]\n",
            " [0.61410496]]\n",
            "에러: \n",
            " 0.08878681402481621\n",
            "+---------EPOCH 1000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.91658292]\n",
            " [0.36458715]\n",
            " [0.92264135]]\n",
            "에러: \n",
            " 0.004653996157757957\n",
            "+---------EPOCH 2000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.9208668 ]\n",
            " [0.36266393]\n",
            " [0.91689461]]\n",
            "에러: \n",
            " 0.004105228792044443\n",
            "+---------EPOCH 3000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92299619]\n",
            " [0.36183207]\n",
            " [0.91443433]]\n",
            "에러: \n",
            " 0.0038662965205599926\n",
            "+---------EPOCH 4000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92488749]\n",
            " [0.36088798]\n",
            " [0.91269919]]\n",
            "에러: \n",
            " 0.003681818226952254\n",
            "+---------EPOCH 5000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92684465]\n",
            " [0.36000853]\n",
            " [0.91054078]]\n",
            "에러: \n",
            " 0.0034790672406160316\n",
            "+---------EPOCH 6000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92685943]\n",
            " [0.36127375]\n",
            " [0.90749191]]\n",
            "에러: \n",
            " 0.0033395642642349618\n",
            "+---------EPOCH 7000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92336329]\n",
            " [0.36401733]\n",
            " [0.90897934]]\n",
            "에러: \n",
            " 0.003592548203894095\n",
            "+---------EPOCH 8000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92257479]\n",
            " [0.3638548 ]\n",
            " [0.91152851]]\n",
            "에러: \n",
            " 0.003752911674675145\n",
            "+---------EPOCH 9000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92239308]\n",
            " [0.36390556]\n",
            " [0.91194685]]\n",
            "에러: \n",
            " 0.0037827367165428074\n",
            "+---------EPOCH 10000---------+\n",
            "입력: \n",
            " [[1.    1.   ]\n",
            " [0.25  0.625]\n",
            " [0.875 0.75 ]]\n",
            "타겟출력: \n",
            " [[1.        ]\n",
            " [0.35714286]\n",
            " [0.83928571]]\n",
            "예측: \n",
            " [[0.92226356]\n",
            " [0.36399293]\n",
            " [0.91211178]]\n",
            "에러: \n",
            " 0.0037978381910849734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcKdSR8Ub9XY"
      },
      "source": [
        "출력 결과로 반복 수가 1-5회일 때의 에러와 이후 1000회 마다의 에러를 볼 수 있는데요.<br/>\n",
        "점점 에러가 줄어드는 경향성을 보이는 것을 확인할 수 있습니다 :)"
      ]
    }
  ]
}